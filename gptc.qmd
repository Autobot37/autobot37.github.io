---
title: "Gpt2.c"
image: "gptc.jpg"
date: 19-05-2024
description: Gpt2 hyper optimisation
---

<body>
<div class="gradient-text">
   <span style="font-size: 1.7rem; line-height: 1.8; font-family: 'Sans Serif';">
        GPT2 hyper optimised in Cuda
    </span>
    <span style="font-size: 1.1rem; line-height: 1.8; font-family: 'Comic Sans'; ">
        <a href="https://github.com/Autobot37/gpt.cpp" target="_blank">Github Link</a>
    </span>
</div>
</body>

# GPT2 implementation in C and Cuda.
{{< video gptc.webm >}}

# Tasks did
- Separate kernels for Cuda and Cpu.
- All kernels from scratch except matmul.
- tokenizer implementation.
- inference implementation.

# Todo
- Optimizing Attention_forward kernel.
- profiling and further optimizing based on profile.

# To run
```bash
git clone -b cuda https://github.com/Autobot37/gpt.cpp
python3 pythonscripts/prepare_tokenizer.py
python3 writestate.py
make run_cuda
```

# Dependencies
- python tiktoken module.
- nvidia cuda toolkit.
